{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PythonでAPIを叩く"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse .bibtex\n",
    "\n",
    "中身はシンプルに\n",
    "```\n",
    "@inproceedings{vaswani2017attention,\n",
    "  title={Attention is all you need},\n",
    "  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\\L}ukasz and Polosukhin, Illia},\n",
    "  booktitle={Advances in neural information processing systems},\n",
    "  pages={5998--6008},\n",
    "  year={2017}\n",
    "}\n",
    "```\n",
    "みたいなbibがいくつか入ってる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref : vaswani2017attention\n",
      "type : inproceedings\n",
      "title : Attention is all you need\n",
      "author : Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \\Lukasz and Polosukhin, Illia\n",
      "booktitle : Advances in neural information processing systems\n",
      "pages : 5998--6008\n",
      "year : 2017\n",
      "\n",
      "ref : lecun2015deep\n",
      "type : article\n",
      "title : Deep learning\n",
      "author : LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey\n",
      "journal : nature\n",
      "volume : 521\n",
      "number : 7553\n",
      "pages : 436--444\n",
      "year : 2015\n",
      "publisher : Nature Publishing Group\n",
      "\n",
      "ref : gers1999learning\n",
      "type : article\n",
      "title : Learning to forget: Continual prediction with LSTM\n",
      "author : Gers, Felix A and Schmidhuber, J\\\"urgen and Cummins, Fred\n",
      "year : 1999\n",
      "publisher : IET\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "from collections import defaultdict\n",
    "\n",
    "# bibtexをdictのlistにparseする\n",
    "def parse_bibtex(fname):\n",
    "    # Open bibtex file\n",
    "    with open(fname) as f:\n",
    "        lines = ''.join(f.readlines()).rstrip()\n",
    "        \n",
    "    # split bibtex into chunks of individual refs\n",
    "    chunks = list(filter(None, lines.split('@')))\n",
    "    \n",
    "    # list of dicts to store the parsed refs\n",
    "    bibs = [defaultdict(lambda : '') for _ in chunks]\n",
    "\n",
    "    # Loop over each chunk\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        \n",
    "        # Get the first ref \n",
    "        match = re.search(r'{(.*?),', chunk)\n",
    "        if match:\n",
    "            bib_ref = match.group(1)\n",
    "            bibs[idx]['ref'] = bib_ref\n",
    "            bibs[idx]['type'] = chunk[:chunk.index('{')]\n",
    "            \n",
    "            i = chunk.index('{') + len(bib_ref)\n",
    "            while i <= len(chunk):\n",
    "                # Capture LHS of =\n",
    "                m = re.search(r'(\\w*?)=', chunk[i:])\n",
    "                \n",
    "                # Capture RHS of =\n",
    "                if m:\n",
    "                    # Make a stack to parse parenthesis\n",
    "                    stack = []\n",
    "                    \n",
    "                    # Push the first element manually.\n",
    "                    i += chunk[i:].index('{')+1\n",
    "                    stack.append(0)\n",
    "                    \n",
    "                    # Loop and parse\n",
    "                    while len(stack) > 0:\n",
    "                        c = chunk[i]\n",
    "                        if c == '{':\n",
    "                            stack.append(0)\n",
    "                        elif c == '}':\n",
    "                            stack.pop()\n",
    "                        else:\n",
    "                            bibs[idx][m.group(1)] += c\n",
    "                        i += 1\n",
    "                i += 1\n",
    "    return bibs\n",
    "            \n",
    "bibs = parse_bibtex('sample.bib')\n",
    "\n",
    "for bib in bibs:\n",
    "    for key, value in bib.items():\n",
    "        print(key, ':', value)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bibtexを元に検索する\n",
    "\n",
    "以下の3つの内どれかが必要\n",
    "\n",
    "- Arxiv ID\n",
    "- DOI\n",
    "- PMC\n",
    "- Pubmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arxiv ID の取得 (Arxiv API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results for: \"Attention is all you need\"\n",
      "\t Attention Is All You http://arxiv.org/abs/1706.03762v5 1706.03762v5\n",
      "\t Attention is all you http://arxiv.org/abs/1906.02792v1 1906.02792v1\n",
      "\t You May Not Need Att http://arxiv.org/abs/1810.13409v1 1810.13409v1\n",
      "Search results for: \"Deep learning\"\n",
      "\t Deep Prior http://arxiv.org/abs/1712.05016v2 1712.05016v2\n",
      "\t Deep Learning http://arxiv.org/abs/1807.07987v2 1807.07987v2\n",
      "\t Deep Graphs http://arxiv.org/abs/1806.01235v1 1806.01235v1\n",
      "Search results for: \"Learning to forget: Continual prediction with LSTM\"\n",
      "\t Learning To Score Ol http://arxiv.org/abs/1611.05125v3 1611.05125v3\n",
      "\t Learning to Skim Tex http://arxiv.org/abs/1704.06877v2 1704.06877v2\n",
      "\t Learning Natural Lan http://arxiv.org/abs/1512.08849v2 1512.08849v2\n"
     ]
    }
   ],
   "source": [
    "import arxiv, os\n",
    "\n",
    "q = 'ti:{0}'\n",
    "for bib in bibs:\n",
    "    query =  q.format(\n",
    "        bib.get('title'),\n",
    "    )\n",
    "    res = arxiv.query(\n",
    "        query=query,\n",
    "        max_results=3,\n",
    "        sort_by=\"relevance\")\n",
    "    print('Search results for: \"{}\"'.format(bib.get('title')))\n",
    "    for r in res:\n",
    "        print('\\t', r.get('title')[:20], r.get('id'), os.path.basename(r.get('id')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOIの取得 (Science Direct API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elsapy.elsclient import ElsClient\n",
    "from elsapy.elssearch import ElsSearch\n",
    "import json\n",
    "    \n",
    "## Load configuration\n",
    "with open(\"config.json\") as conf:\n",
    "    config = json.load(conf)\n",
    "\n",
    "## Initialize client\n",
    "client = ElsClient(config['apikey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results for: \"ti:Attention is all you need\"\n",
      "\tOops, Some sort of error\n",
      "Search results for: \"ti:Deep learning\"\n",
      "\tOops, Some sort of error\n",
      "Search results for: \"ti:Learning to forget: Continual prediction with LSTM\"\n",
      "\tOops, Some sort of error\n"
     ]
    }
   ],
   "source": [
    "## Initialize doc search object using ScienceDirect and execute search, \n",
    "#   retrieving all results\n",
    "\n",
    "for bib in bibs:\n",
    "    query =  q.format(bib.get('title'))\n",
    "    print('Search results for: \"{}\"'.format(query))\n",
    "    try:\n",
    "        doc_srch = ElsSearch(query,'sciencedirect')\n",
    "        doc_srch.execute(client, get_all = False)\n",
    "        res = doc_srch.results_df\n",
    "\n",
    "        res= doc_srch.results_df\n",
    "        for _, r in res.iterrows():\n",
    "            print('\\t', r.get('dc:title')[:30], r.get('dc:identifier'))\n",
    "            print('\\t\\t', r.get('prism:url'))\n",
    "            if _ > 3: break\n",
    "    except:\n",
    "        print('\\tOops, Some sort of error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOIの取得 (cross ref API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search results for: \"ti:Attention is all you need\"\n",
      "\t Attention All Survey 10.1108/02630809410074466\n",
      "\t\t http://dx.doi.org/10.1108/02630809410074466\n",
      "\t Attention Is (not) A 10.18653/v1/p19-1477\n",
      "\t\t http://dx.doi.org/10.18653/v1/p19-1477\n",
      "\t Knowledge Is All You 10.1111/phis.12060\n",
      "\t\t http://dx.doi.org/10.1111/phis.12060\n",
      "\t All You Need Is Love 10.1093/oso/9780190460723.003.0003\n",
      "\t\t http://dx.doi.org/10.1093/oso/9780190460723.003.0003\n",
      "\t All you need for all 10.1038/bdjteam.2016.20\n",
      "\t\t http://dx.doi.org/10.1038/bdjteam.2016.20\n",
      "Search results for: \"ti:Deep learning\"\n",
      "\t Deep drawability of  10.1063/1.5008085\n",
      "\t\t http://dx.doi.org/10.1063/1.5008085\n",
      "\t Computer Vision, Dee 10.14293/s2199-1006.1.sor-uncat.clzwyuz.v1\n",
      "\t\t http://dx.doi.org/10.14293/s2199-1006.1.sor-uncat.clzwyuz.v1\n",
      "\t Introduction to Deep 10.7551/mitpress/11171.003.0004\n",
      "\t\t http://dx.doi.org/10.7551/mitpress/11171.003.0004\n",
      "\t A Brief History of D 10.7551/mitpress/11171.003.0007\n",
      "\t\t http://dx.doi.org/10.7551/mitpress/11171.003.0007\n",
      "\t The Future of Deep L 10.7551/mitpress/11171.003.0010\n",
      "\t\t http://dx.doi.org/10.7551/mitpress/11171.003.0010\n",
      "Search results for: \"ti:Learning to forget: Continual prediction with LSTM\"\n",
      "\t Learning to forget:  10.1049/cp:19991218\n",
      "\t\t http://dx.doi.org/10.1049/cp:19991218\n",
      "\t Faculty of 1000 eval 10.3410/f.1006034.75155\n",
      "\t\t http://dx.doi.org/10.3410/f.1006034.75155\n",
      "\t Learning to Forget:  10.1162/089976600300015015\n",
      "\t\t http://dx.doi.org/10.1162/089976600300015015\n",
      "\t Continual Prediction 10.1007/978-1-4471-0877-1_10\n",
      "\t\t http://dx.doi.org/10.1007/978-1-4471-0877-1_10\n",
      "\t Continual Representa 10.5220/0007687103670373\n",
      "\t\t http://dx.doi.org/10.5220/0007687103670373\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from crossref.restful import Works\n",
    "\n",
    "works = Works()\n",
    "\n",
    "for bib in bibs:\n",
    "    query =  q.format(bib.get('title'))\n",
    "    print('Search results for: \"{}\"'.format(query))\n",
    "    res = iter(works.query(query).sort())\n",
    "    try:\n",
    "        for _, r in enumerate(res):\n",
    "            print('\\t', r.get('title')[0][:20], r.get('DOI'))\n",
    "            print('\\t\\t', r.get('URL'))\n",
    "            if _ > 3: break\n",
    "    except:\n",
    "        print('\\tOops, Some sort of error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReferencesTo and ReferencesFrom の取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ProphyAPIに関するドキュメントはないし，継続して使える保証がないのは心配．\n",
    "\n",
    "GraphQLを使ってるみたいで，https://www.prophy.science/api/graphql/ からちょっとしたドキュメントが見れる．\n",
    "\n",
    "Auth関係がないみたいだったから，Prophy管理者に使用許可→取得済み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{\n",
      "  \"msg\": \"Welcome to Prophy API\", \n",
      "  \"time\": \"Wed, 22 Jan 2020 16:33:10 GMT\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "\n",
    "r = requests.get(\"https://www.prophy.science/api\")\n",
    "print(r) # Response 200 = OK\n",
    "print(r.content.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.prophy.science/api/graphql'\n",
    "headers= {'Accept': 'application/vnd.cap-collectif.preview+json'}\n",
    "queries = {'query' :'query {article(id:430) {id}}'}\n",
    "\n",
    "r = requests.post(url, headers=headers, json=queries)\n",
    "id = json.loads(r.content).get('data').get('article').get('id')\n",
    "print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.prophy.science/api/articles/{0}/full'.format(id)\n",
    "queries = {'query' :'query {article(id:430) {id}}'}\n",
    "\n",
    "r = requests.get(url, headers=headers, json=queries)\n",
    "res = json.loads(r.content)\n",
    "referencesFrom = res['referencesFrom']\n",
    "referencesTo = res['referencesTo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Towards a Better Understanding ...\n",
      "Neonatal Resveratrol and Nicot ...\n",
      "Perilipin-2 deletion promotes  ...\n",
      "Genetics of metabolic syndrome ...\n",
      "Adipocyte Liver Kinase b1 Supp ...\n",
      "Human brown adipose tissue as  ...\n",
      "Adipose tissue macrophages dev ...\n",
      "Targeting PPAR γ  in the epige ...\n",
      "Global IP6K1 deletion enhances ...\n",
      "PDGFRα controls the balance of ...\n"
     ]
    }
   ],
   "source": [
    "for item in referencesTo['references']:\n",
    "    print(item['title'][:30], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ectopic brown adipose tissue i ...\n",
      "Inactivation of the Mitochondr ...\n",
      "Allelic forms of a gene contro ...\n",
      "Innervation of mammalian white ...\n",
      "The Uncoupling Protein 1 Gene  ...\n",
      "The landscape of genetic compl ...\n",
      "Brown adipose tissue hyperplas ...\n",
      "Brown adipose tissue: function ...\n",
      "Tissue-specific and beta-adren ...\n",
      "FOXC2 is a winged helix gene t ...\n",
      "Beta 3-adrenergic receptor sti ...\n",
      "RIP140-Targeted Repression of  ...\n",
      "Adipocyte differentiation and  ...\n",
      "Strain-Specific Response to  3 ...\n",
      "Dietary fat interacts with QTL ...\n",
      "Identification and Importance  ...\n",
      "Mice lacking mitochondrial unc ...\n",
      "Genetic regulation of traits e ...\n",
      "Ontogeny and perinatal modulat ...\n",
      "Emergence of brown adipocytes  ...\n",
      "Target of rapamycin-mediated a ...\n",
      "Regulatory circuits controllin ...\n",
      "Effect of CL-316,243, a thermo ...\n",
      "Multilocular fat cells in WAT  ...\n",
      "In developing brown adipose ti ...\n",
      "The LIS1-related protein NUDF  ...\n",
      "Uncoupling protein in embryoni ...\n",
      "Transcriptional Control of Bro ...\n",
      "Regulation of the brown and wh ...\n",
      "Peroxisome proliferator–activa ...\n",
      "Synergistic gene interactions  ...\n",
      "Brown Fat and the Myth of Diet ...\n",
      "The genetics of brown adipose  ...\n",
      "An upstream enhancer regulatin ...\n",
      "Nuclear receptor corepressor R ...\n",
      "Genetic Complexity and Quantit ...\n",
      "A cold-inducible coactivator o ...\n",
      "A role for brown adipose tissu ...\n",
      "PRDM16 Controls a Brown Fat/Sk ...\n",
      "Transcriptional Control of Bro ...\n",
      "Differentiation-dependent expr ...\n",
      "Glycogen synthase: a putative  ...\n",
      "Gene expression analysis of mo ...\n",
      "Brown fat and thermogenesis. ...\n",
      "Flow cytometric and immunohist ...\n",
      "Noradrenaline turnover in brow ...\n",
      "Cold-Activated Brown Adipose T ...\n",
      "Functional Brown Adipose Tissu ...\n",
      "Transcriptional Synergy and th ...\n",
      "Genetic variability affects th ...\n",
      "“Recombinant inbred strains,”, ...\n"
     ]
    }
   ],
   "source": [
    "for item in referencesFrom['references']:\n",
    "    print(item['title'][:30], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
